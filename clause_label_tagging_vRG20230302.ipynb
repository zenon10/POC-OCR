{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XuVVIa-Z-f0j"
      ],
      "authorship_tag": "ABX9TyMfwu6APbvZkQl6t9C2y3DG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zenon10/POC-OCR/blob/main/clause_label_tagging_vRG20230302.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgM48P5KQWBR",
        "outputId": "5654830f-d765-45b0-dab0-6ed09f0069e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "import pandas as pd \n",
        "from unidecode import unidecode\n",
        "import numpy as np\n",
        "import json"
      ],
      "metadata": {
        "id": "F_Ajd-xkNN4S"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def excel_keyword_parser( keyword_excel):\n",
        "    '''\n",
        "    Make dict for all keywords with respected clause label\n",
        "    input : keyword excel\n",
        "    output : return dict\n",
        "    '''\n",
        "    df = pd.read_excel(keyword_excel,sheet_name = 'Clauses')\n",
        "    df_fr = df[['Clause Type in English','Keywords in Native Language']].copy()\n",
        "    df_fr.iloc[:,1] = df_fr.iloc[:,1].str.replace(';',',').replace(['\"','-'],' ',regex= True).str.split(',')\n",
        "    clause_kw_dict = dict(list(zip(df_fr.iloc[:,0],df_fr.iloc[:,1])))\n",
        "    return clause_kw_dict"
      ],
      "metadata": {
        "id": "pUijB-YMKsbp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_f-1NBesKhe8"
      },
      "outputs": [],
      "source": [
        "#for matching 1 word\n",
        "def match_word(word,clause):\n",
        "    '''\n",
        "    Match word with clause\n",
        "    input : word from list of keyword and clause\n",
        "    output : flag if keyword match\n",
        "    '''\n",
        "    count = 0\n",
        "    if '&' in word:\n",
        "        words = word.split('&')\n",
        "        for j in words: \n",
        "            if clause.__contains__(j):\n",
        "                count = count + 1\n",
        "        flag = word if count == len(words) else None\n",
        "    else:\n",
        "        flag = word if clause.__contains__(word) else None\n",
        "    return flag\n",
        "\n",
        "\n",
        "#Matching a keywords list from clause \n",
        "def match_keyword_to_clause(keyword_list,clause):\n",
        "    '''\n",
        "    Match list of keywords with clause\n",
        "    input : list of keyword and clause\n",
        "    output : return sum of keyword present\n",
        "    '''\n",
        "    keyword = [unidecode(j.strip().lower()) for j in keyword_list]\n",
        "    clause = unidecode(clause.lower().replace('-',' '))\n",
        "    clause = unidecode(clause.lower().replace('\\n',' '))\n",
        "    cleaned_clause = ' '.join(clause.splitlines())\n",
        "    key_match = []\n",
        "    for i in keyword:\n",
        "        key_match.append(match_word(i,cleaned_clause))\n",
        "    match_key = list(filter(None.__ne__, key_match))\n",
        "    return match_key\n",
        "\n",
        "def para_clause_tagging(clause, clause_kw_dict):\n",
        "    '''\n",
        "    Match clause with keyword\n",
        "    input : clause and dict of keyword\n",
        "    output : return clause label with max keyword\n",
        "    '''\n",
        "    d = dict((k, match_keyword_to_clause(v,clause)) for k, v in clause_kw_dict.items())\n",
        "    max_length = 0\n",
        "    max_key_value = None\n",
        "\n",
        "    for key, value in d.items():\n",
        "        if isinstance(value, list) and len(value) > max_length:\n",
        "            max_length = len(value)\n",
        "            max_key_value = (key, value)\n",
        "\n",
        "    return max_key_value     "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## saving result as excel "
      ],
      "metadata": {
        "id": "XuVVIa-Z-f0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clause_type_excel(excel_file,clause_kw_dict):\n",
        "    df1 = pd.read_excel(excel_file)\n",
        "    df1['section_id'].fillna('s0',inplace=True)\n",
        "\n",
        "    #gives paragraph level clause type\n",
        "    df1['para_level_clause_type'] = df1.iloc[:,1].apply(lambda x: para_clause_tagging(str(x),clause_kw_dict)[0] if para_clause_tagging(str(x),clause_kw_dict) is not None else None)\n",
        "    df1['para_level_clause_type_keyword'] = df1.iloc[:,1].apply(lambda x: para_clause_tagging(str(x),clause_kw_dict)[1] if para_clause_tagging(str(x),clause_kw_dict) is not None else None)\n",
        "\n",
        "    #combine text with identical section id\n",
        "    sec1 = df1.groupby('section_id')['paragraph'].apply(lambda x: (' '.join(x))).reset_index()\n",
        " \n",
        "    sec1['section_level_clause_type'] = sec1['paragraph'].apply(lambda x: para_clause_tagging(x,clause_kw_dict)[0] if para_clause_tagging(x,clause_kw_dict) is not None else None)\n",
        "    sec1['section_level_clause_type_keyword'] = sec1['paragraph'].apply(lambda x: para_clause_tagging(x,clause_kw_dict)[1] if para_clause_tagging(x,clause_kw_dict) is not None else None)\n",
        "    df_sec = pd.merge(df1,sec1.drop('paragraph', axis =1),on =['section_id'],how ='outer') \n",
        "    \n",
        "    df_ab = df_sec.dropna(subset=['section_level_clause_type'], how='all')\n",
        "    section = df_ab['section_id'].unique().tolist()\n",
        "    result = pd.DataFrame(columns = ['section_id','clause_type','para_id','clause'])\n",
        "    for i in section:\n",
        "        test = df_ab[df_ab['section_id']==i]\n",
        "        if test['para_level_clause_type'].nunique() ==1:\n",
        "            if test['para_level_clause_type'].dropna().unique() == test['section_level_clause_type'].unique():\n",
        "                id = test['para_id'].tolist()\n",
        "                section_id = i\n",
        "                clause = \" \".join(test['paragraph'])\n",
        "                clause_type = test['section_level_clause_type'].unique()\n",
        "                result.loc[len(result.index)] = [section_id,clause_type,id,clause]\n",
        "        else:\n",
        "            test.dropna(subset=['para_level_clause_type'],how='all',inplace = True)\n",
        "            test['kw_str'] = test.para_level_clause_type_keyword.apply(lambda x: ''.join(x))\n",
        "            test['cond_match'] = np.where(test.para_level_clause_type != test.section_level_clause_type, np.where(test.kw_str.str.find('&'or ' ') ==-1,None, \"keep\"), \"keep\")\n",
        "            test.dropna(subset=['cond_match'],how='all',inplace = True)\n",
        "            g = test.groupby(['section_id','para_level_clause_type']).agg({'para_id':lambda x: x.tolist(),'paragraph': lambda x:' '.join(x)})\n",
        "            g.reset_index(inplace= True)\n",
        "            result = pd.concat([result,g.rename(columns={'paragraph':'clause','para_level_clause_type':'clause_type'})])\n",
        "    result.to_excel(\"clause_with_label1.xlsx\")"
      ],
      "metadata": {
        "id": "XMUE5ER0Kw-x"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clause_kw_dict = excel_keyword_parser(\"Clause Management_Deep Dive France_Mapping Document.xlsx\")\n",
        "get_clause_type_excel('s04_cctp_montpellier v2_ocr1.xlsx',clause_kw_dict)"
      ],
      "metadata": {
        "id": "up5-c83UK2Wn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9LTr9zziK3Q2"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## saving result as json"
      ],
      "metadata": {
        "id": "L8YKG6k2-oCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clause_type(excel_file,clause_kw_dict):\n",
        "    df1 = pd.read_excel(excel_file)\n",
        "    df1['section_id'].fillna('s0',inplace=True)\n",
        "\n",
        "    #gives paragraph level clause type\n",
        "    df1['para_level_clause_type'] = df1.iloc[:,1].apply(lambda x: para_clause_tagging(str(x),clause_kw_dict)[0] if para_clause_tagging(str(x),clause_kw_dict) is not None else None)\n",
        "    df1['para_level_clause_type_keyword'] = df1.iloc[:,1].apply(lambda x: para_clause_tagging(str(x),clause_kw_dict)[1] if para_clause_tagging(str(x),clause_kw_dict) is not None else None)\n",
        "\n",
        "    #combine text with identical section id\n",
        "    sec1 = df1.groupby('section_id')['paragraph'].apply(lambda x: (' '.join(x))).reset_index()\n",
        " \n",
        "    sec1['section_level_clause_type'] = sec1['paragraph'].apply(lambda x: para_clause_tagging(x,clause_kw_dict)[0] if para_clause_tagging(x,clause_kw_dict) is not None else None)\n",
        "    sec1['section_level_clause_type_keyword'] = sec1['paragraph'].apply(lambda x: para_clause_tagging(x,clause_kw_dict)[1] if para_clause_tagging(x,clause_kw_dict) is not None else None)\n",
        "    df_sec = pd.merge(df1,sec1.drop('paragraph', axis =1),on =['section_id'],how ='outer') \n",
        "    df_sec.to_excel(\"extract_clause_type_\" + excel_file)\n",
        "\n",
        "    df_ab = df_sec.dropna(subset=['section_level_clause_type'], how='all')\n",
        "    section = df_ab['section_id'].unique().tolist()\n",
        "    result = pd.DataFrame(columns = ['value','clause_type'])\n",
        "    for i in section:\n",
        "        test = df_ab[df_ab['section_id']==i]\n",
        "        if test['para_level_clause_type'].nunique() ==1:\n",
        "            if test['para_level_clause_type'].dropna().unique() == test['section_level_clause_type'].unique():\n",
        "                clause = \" \".join(test['paragraph'])\n",
        "                clause_type = test['section_level_clause_type'].unique()[0]\n",
        "                result.loc[len(result.index)] = [clause,clause_type]\n",
        "        else:\n",
        "            test.dropna(subset=['para_level_clause_type'],how='all',inplace = True)\n",
        "            test['kw_str'] = test.para_level_clause_type_keyword.apply(lambda x: ''.join(x))\n",
        "            test['cond_match'] = np.where(test.para_level_clause_type != test.section_level_clause_type, np.where(test.kw_str.str.find('&'or ' ') ==-1,None, \"keep\"), \"keep\")\n",
        "            test.dropna(subset=['cond_match'],how='all',inplace = True)\n",
        "            g = test.groupby(['para_level_clause_type']).agg({'paragraph': lambda x:' '.join(x)})\n",
        "            g.reset_index(inplace= True)\n",
        "            result = pd.concat([result,g.rename(columns={'paragraph':'value','para_level_clause_type':'clause_type'})],ignore_index = True)\n",
        "    result.reset_index(inplace = True)\n",
        "    result['confidence'] = 0\n",
        "    result['file_name'] = excel_file\n",
        "    out = result.to_dict(orient = 'records')\n",
        "    with open('clauses.json','w') as f:\n",
        "      json.dump({'clause': out},f, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "_k-UnuGM-nHh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clause_kw_dict = excel_keyword_parser(\"Clause Management_Deep Dive France_Mapping Document.xlsx\")\n",
        "get_clause_type('s04_cctp_montpellier v2_ocr1.xlsx',clause_kw_dict)\n"
      ],
      "metadata": {
        "id": "VZBv_HKaNNYe"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}